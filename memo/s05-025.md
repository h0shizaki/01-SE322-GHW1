# S05

## url

https://levelup.gitconnected.com/preventing-data-inconsistencies-in-mysql-strategies-for-avoiding-lost-updates-cfdb04107f7c

## archive url

## Source type

blocker

## Author type

yoonsin91

## Date

Mar 30

## Problem

1. lost updates : 2 transaction attempt to update the same data, but only one of the successful
2. dirty reads : transaction reads uncommitted changes made by another transaction
3. phantom reads : transactions sees different rows that mtch a condition after another transaction adds or removes rows that also meets the same condition.

## Solution for problem 1

1. use atomic operations
2. use lock
3. repeatable read is required to perevent lost updates, which use shared locks to prevent other transaction from modifying the data

## Solution for problem 2

read committed level : only allowing a transaction to read changes that have been commited by other transactions.

## Decision Driver

repeatable read level or higher : ensure that a transactions ca read the same set of rows consistetly, even if other transaction are updating the data simultaneouly. (achieved by placing shared locks on all the rows accessd by transaction)

## References

N/A

## Remark

My SQL provides 4 transaction isolation level tht determine how transaction interact with another and how chages are visible to another trnsaction

1. READ UNCOMMITTED
2. READ COMMITTED
3. REPEATABLE READ
4. SERIALIZABLE
